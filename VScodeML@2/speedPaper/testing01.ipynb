{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "from keras import layers, Model\n",
    "from sklearn.metrics import mean_absolute_percentage_error, root_mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ETL:\n",
    "    \"\"\"\n",
    "    ticker: str\n",
    "    period: string\n",
    "    test_size: float betwee 0 and 1\n",
    "    n_input: int\n",
    "    timestep: int\n",
    "    Extracts data for stock with ticker `ticker` from yf api,\n",
    "    splits the data into train and test sets by date,\n",
    "    reshapes the data into np.array of shape [#weeks, 5, 1],\n",
    "    converts our problem into supervised learning problem.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datainput, test_size=0.2, n_input=5, timestep=5, normalize_bool=False, scaler=None) -> None:\n",
    "        self.datainput = datainput\n",
    "        self.test_size = test_size\n",
    "        self.n_input = n_input\n",
    "        self.timestep = timestep\n",
    "        self.normalize_bool = normalize_bool\n",
    "        self.scaler = scaler\n",
    "        self.df = self.extract_historic_data()\n",
    "        self.train, self.test = self.etl()\n",
    "        self.X_train, self.y_train = self.to_supervised(self.train)\n",
    "        self.X_test, self.y_test = self.to_supervised(self.test)\n",
    "\n",
    "        if self.normalize_bool:\n",
    "            print('normalized', normalize_bool)\n",
    "            self.scaler = scaler\n",
    "        else:\n",
    "            print('not normalized', normalize_bool)\n",
    "            self.scaler = None\n",
    "\n",
    "    def extract_historic_data(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Extracts historic data and optionally normalizes it.\n",
    "        \"\"\"\n",
    "        # data = self.datainput\n",
    "        if self.normalize_bool:\n",
    "            data = self.scaler.fit_transform(self.datainput.values)\n",
    "            return pd.DataFrame(data, columns=self.datainput.columns)\n",
    "        else:\n",
    "            return self.datainput\n",
    "\n",
    "    def split_data(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Splits our pd.Series into train and test series with\n",
    "        test series representing test_size * 100 % of data.\n",
    "        \"\"\"\n",
    "        # data = self.extract_historic_data()\n",
    "        data = self.df\n",
    "        print(\"data shape:\", data.shape)\n",
    "        if len(data) != 0:\n",
    "            train_idx = round(len(data) * (1-self.test_size))\n",
    "            train = data[:train_idx]\n",
    "            test = data[train_idx:]\n",
    "            # train = np.array(train)\n",
    "            # test = np.array(test)\n",
    "            # return train[:, np.newaxis], test[:, np.newaxis]\n",
    "            return train.values, test.values\n",
    "        else:\n",
    "            raise Exception('Data set is empty, cannot split.')\n",
    "\n",
    "    def window_and_reshape(self, data) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformats data into shape our model needs,\n",
    "        namely, [# samples, timestep, # feautures]\n",
    "        samples\n",
    "        \"\"\"\n",
    "        # NUM_FEATURES = 1\n",
    "        samples = int(data.shape[0] // self.timestep)\n",
    "        # result = np.array(np.array_split(data, samples))\n",
    "        # output = result.reshape((samples, self.timestep, NUM_FEATURES))\n",
    "        trimmed_data = data[:samples * self.timestep]\n",
    "        reshaped_data = trimmed_data.reshape(\n",
    "            (samples, self.timestep, data.shape[1]))\n",
    "        print(reshaped_data.shape)\n",
    "        # print(output.shape)\n",
    "        return reshaped_data\n",
    "\n",
    "    def transform(self, train, test) -> np.array:\n",
    "        train_remainder = train.shape[0] % self.timestep\n",
    "        test_remainder = test.shape[0] % self.timestep\n",
    "        # if train_remainder != 0:\n",
    "        #     train = train[train_remainder:]\n",
    "        # if test_remainder != 0:\n",
    "        #     test = test[test_remainder:]\n",
    "        if train_remainder != 0:\n",
    "            train = train[:-train_remainder]\n",
    "        if test_remainder != 0:\n",
    "            test = test[:-test_remainder]\n",
    "        print(\"Train shape:\", train.shape)\n",
    "        print(\"Test shape:\", test.shape)\n",
    "        # print(\"train:\", train, \"test:\", test)\n",
    "        return self.window_and_reshape(train), self.window_and_reshape(test)\n",
    "\n",
    "    def etl(self) -> tuple[np.array, np.array]:\n",
    "        \"\"\"\n",
    "        Runs complete ETL\n",
    "        \"\"\"\n",
    "        train, test = self.split_data()\n",
    "        print(\"train shape:\", train.shape, \"test shape:\", test.shape)\n",
    "        return self.transform(train, test)\n",
    "\n",
    "    def to_supervised(self, data, n_out=5) -> tuple:\n",
    "        \"\"\"\n",
    "        Converts our time series prediction problem to a\n",
    "        supervised learning problem.\n",
    "        \"\"\"\n",
    "        # flatted the data\n",
    "        data = data.reshape((data.shape[0] * data.shape[1], data.shape[2]))\n",
    "        X, y = [], []\n",
    "        in_start = 0\n",
    "        # step over the entire history one time step at a time\n",
    "        for _ in range(len(data)):\n",
    "            # define the end of the input sequence\n",
    "            in_end = in_start + self.n_input\n",
    "            out_end = in_end + n_out\n",
    "            # ensure we have enough data for this instance\n",
    "            if out_end <= len(data):\n",
    "                x_input = data[in_start:in_end, 0]\n",
    "                x_input = x_input.reshape((len(x_input), 1))\n",
    "                X.append(x_input)\n",
    "                y.append(data[in_end:out_end, 0])\n",
    "                # move along one time step\n",
    "                in_start += 1\n",
    "        print(np.array(X), np.array(y))\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "# *******************************\n",
    "# Implementing a Transformer\n",
    "\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.2, epsilon=1e-6, attention_axes=None, kernel_size=1):\n",
    "    \"\"\"\n",
    "    Creates a single transformer block.\n",
    "    \"\"\"\n",
    "    x = layers.LayerNormalization(epsilon=epsilon)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout,\n",
    "        attention_axes=attention_axes\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=epsilon)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=kernel_size,\n",
    "                      activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=kernel_size)(x)\n",
    "    return x + res\n",
    "\n",
    "\n",
    "def build_transfromer(head_size, num_heads, ff_dim, num_trans_blocks, mlp_units, dropout=0.2, mlp_dropout=0.2, attention_axes=None, epsilon=1e-6, kernel_size=1):\n",
    "    \"\"\"\n",
    "    Creates final model by building many transformer blocks.\n",
    "    \"\"\"\n",
    "    n_timesteps, n_features, n_outputs = 5, 3, 5\n",
    "    inputs = tf.keras.Input(shape=(n_timesteps, n_features))\n",
    "    x = inputs\n",
    "    for _ in range(num_trans_blocks):\n",
    "        x = transformer_encoder(x, head_size=head_size, num_heads=num_heads, ff_dim=ff_dim,\n",
    "                                dropout=dropout, attention_axes=attention_axes, kernel_size=kernel_size, epsilon=epsilon)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "    outputs = layers.Dense(n_outputs)(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def fit_transformer(transformer: tf.keras.Model, x_train, y_train, X_val, y_val, display_loss=False):\n",
    "    \"\"\"\n",
    "    Compiles and fits our transformer.\n",
    "    \"\"\"\n",
    "    transformer.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        metrics=[\"mae\", 'mape'])\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss', patience=10, restore_best_weights=True)]\n",
    "    # hist = transformer.fit(data.X_train, data.y_train, batch_size=32, epochs=25, verbose=1, callbacks=callbacks)\n",
    "    hist = transformer.fit(x_train, y_train, validation_data=(X_val, y_val),\n",
    "                           batch_size=32, epochs=25, verbose=1, callbacks=callbacks)\n",
    "    history_df = pd.DataFrame(hist.history)\n",
    "\n",
    "    if display_loss:\n",
    "        history_df['val_loss'].plot()\n",
    "        plt.title('Validation Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Validation'], loc='upper right')\n",
    "        plt.show()\n",
    "        history_df['loss'].plot()\n",
    "        plt.title('Training Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train'], loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "        # Display minimum validation loss\n",
    "        min_val_loss = history_df['val_loss'].min()\n",
    "        print(\"Minimum validation loss: {:.4f}\".format(min_val_loss))\n",
    "        # Plot mae\n",
    "        history_df.loc[:, ['mae', 'val_mae']].plot()\n",
    "        plt.title('Model MAE')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "        plt.show()\n",
    "        # Display minimum validation mae and mape\n",
    "        min_val_mae = history_df['val_mae'].min()\n",
    "        min_val_mape = history_df['val_mape'].min()\n",
    "        print(\"Minimum validation MAE: {:.4f}\".format(min_val_mae))\n",
    "        print(\"Minimum validation MAPE: {:.4f}\".format(min_val_mape))\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "def recursive_predict(model, input_seq, n_steps, n_features):\n",
    "\n",
    "    predictions = []\n",
    "    current_seq = input_seq\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # Make a prediction\n",
    "        next_pred = model.predict(current_seq)\n",
    "\n",
    "        # Append the prediction to the results\n",
    "        predictions.append(next_pred[0, 0])  # Assuming single output\n",
    "\n",
    "        # Update the current sequence\n",
    "        next_seq = np.append(current_seq[:, 1:, :], [[next_pred]], axis=1)\n",
    "        current_seq = next_seq\n",
    "\n",
    "    return np.array(predictions)\n",
    "# **********************\n",
    "\n",
    "\n",
    "class PredictAndForecast:\n",
    "    \"\"\"\n",
    "    model: tf.keras.Model\n",
    "    train: np.array\n",
    "    test: np.array\n",
    "    Takes a trained model, train, and test datasets and returns predictions\n",
    "    of len(test) with same shape.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, train, test, n_input=5, scaler=None, normalize_bool=False) -> None:\n",
    "        self.model = model\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.n_input = n_input\n",
    "        self.scaler = scaler\n",
    "        self.normalize_bool = normalize_bool\n",
    "        self.predictions = self.get_predictions()\n",
    "\n",
    "    def forecast(self, history) -> np.array:\n",
    "        \"\"\"\n",
    "        Given last weeks actual data, forecasts next weeks prices.\n",
    "        \"\"\"\n",
    "        # flatten data\n",
    "        data = np.array(history)\n",
    "        data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "        # retrieve last observations for input data\n",
    "        input_x = data[-self.n_input:, :]\n",
    "        # reshape into [1, n_input, 1]\n",
    "        input_x = input_x.reshape((1, len(input_x), input_x.shape[1]))\n",
    "        # forecast the next week\n",
    "        yhat = self.model.predict(input_x, verbose=0)\n",
    "        # we only want the vector forecast\n",
    "        yhat = yhat[0]\n",
    "        return yhat\n",
    "\n",
    "    def get_predictions(self) -> np.array:\n",
    "        \"\"\"\n",
    "        compiles models predictions week by week over entire\n",
    "        test set.\n",
    "        \"\"\"\n",
    "        # history is a list of weekly data\n",
    "        history = [x for x in self.train]\n",
    "        # walk-forward validation over each week\n",
    "        predictions = []\n",
    "        for i in range(len(self.test)):\n",
    "            yhat_sequence = self.forecast(history)\n",
    "            # store the predictions\n",
    "            predictions.append(yhat_sequence)\n",
    "            # get real observation and add to history for predicting the next week\n",
    "            history.append(self.test[i, :])\n",
    "        predictions = np.array(predictions)\n",
    "        # if self.normalize_bool:\n",
    "        # Inverse transform to get back to original scale\n",
    "        #    predictions = self.scaler.inverse_transform(\n",
    "        #        predictions.reshape(-1, 1)).reshape(predictions.shape)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class Evaluate:\n",
    "\n",
    "    def __init__(self, actual, predictions, normalize_bool, scaler) -> None:\n",
    "        print(\n",
    "            f\"Initial lengths - actual: {len(actual)}, predictions: {len(predictions)}\")\n",
    "        if normalize_bool == True:\n",
    "            actual = scaler.inverse_transform(\n",
    "                actual.reshape(-1, 1)).reshape(actual.shape)\n",
    "            predictions = scaler.inverse_transform(\n",
    "                predictions.reshape(-1, 1)).reshape(predictions.shape)\n",
    "        self.actual = actual\n",
    "        self.predictions = predictions\n",
    "        self.var_ratio = self.compare_var()\n",
    "        self.mape = self.evaluate_model_with_mape()\n",
    "        self.rmse = self.evaluate_model_with_rmse()\n",
    "        self.mae = self.evaluate_model_with_mae()\n",
    "        self.r2 = self.evaluate_model_with_r2()\n",
    "\n",
    "    def compare_var(self):\n",
    "        return abs(1 - (np.var(self.predictions) / np.var(self.actual)))\n",
    "\n",
    "    def evaluate_model_with_mape(self):\n",
    "        return mean_absolute_percentage_error(self.actual.flatten(), self.predictions.flatten())\n",
    "\n",
    "    def evaluate_model_with_rmse(self):\n",
    "        return root_mean_squared_error(self.actual.flatten(), self.predictions.flatten())\n",
    "\n",
    "    def evaluate_model_with_mae(self):\n",
    "        return mean_absolute_error(self.actual.flatten(), self.predictions.flatten())\n",
    "\n",
    "    def evaluate_model_with_r2(self):\n",
    "        return r2_score(self.actual.flatten(), self.predictions.flatten())\n",
    "\n",
    "\n",
    "def plot_results(train, test, preds, df, normalize_bool, scaler, title_suffix=None, xlabel='AAPL stock Price'):\n",
    "    \"\"\"\n",
    "    Plots training data in blue, actual values in red, and predictions in green,\n",
    "    over time.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "    # x = df.Close[-498:].index\n",
    "    plot_test = test[1:]\n",
    "    plot_preds = preds[1:]\n",
    "    if normalize_bool == True:\n",
    "        # Inverse transform to get back to original scale\n",
    "        plot_test = scaler.inverse_transform(\n",
    "            plot_test.reshape(-1, 1)).reshape(plot_test.shape)\n",
    "        train = scaler.inverse_transform(\n",
    "            train.reshape(-1, 1)).reshape(train.shape)\n",
    "        plot_preds = scaler.inverse_transform(\n",
    "            plot_preds.reshape(-1, 1)).reshape(plot_preds.shape)\n",
    "        print(\"normalization convert active\")\n",
    "\n",
    "    # x = df[-(plot_test.shape[0]*plot_test.shape[1]):].index\n",
    "    plot_test = plot_test.reshape((plot_test.shape[0]*plot_test.shape[1], 1))\n",
    "    plot_preds = plot_preds.reshape((plot_test.shape[0]*plot_test.shape[1], 1))\n",
    "    plot_train = train.reshape((train.shape[0]*train.shape[1], 1))\n",
    "    print(f'plot_train shape: {plot_train.shape}')\n",
    "    print(f'plot_test shape: {plot_test.shape}')\n",
    "    print(f'plot_preds shape: {plot_preds.shape}')\n",
    "\n",
    "    x_train = df[:len(plot_train)].index\n",
    "    x_test = df[len(plot_train):len(plot_train) + len(plot_test)].index\n",
    "\n",
    "    ax.plot(x_train, plot_train, label='Train', color='blue')\n",
    "    ax.plot(x_test, plot_test, label='actual', color='red')\n",
    "    ax.plot(x_test, plot_preds, label='preds', color='green')\n",
    "    if title_suffix == None:\n",
    "        ax.set_title('Predictions vs. Actual')\n",
    "    else:\n",
    "        ax.set_title(f'Predictions vs. Actual, {title_suffix}')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel(xlabel)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_TEMA(data, period=10):\n",
    "    ema1 = data.ewm(span=period, adjust=False).mean()\n",
    "    ema2 = ema1.ewm(span=period, adjust=False).mean()\n",
    "    ema3 = ema2.ewm(span=period, adjust=False).mean()\n",
    "    tema = (3 * ema1) - (3 * ema2) + ema3\n",
    "    return tema.dropna()\n",
    "\n",
    "\n",
    "def calculate_DEMA(data, period=10):\n",
    "    ema1 = data.ewm(span=period, adjust=False).mean()\n",
    "    ema2 = ema1.ewm(span=period, adjust=False).mean()\n",
    "    dema = 2 * ema1 - ema2\n",
    "    return dema\n",
    "\n",
    "\n",
    "ticker = 'AAPL'\n",
    "# timeframe days\n",
    "dataset = yf.download(ticker, start='1990-01-01',\n",
    "                      end=datetime.now().strftime('%Y-%m-%d'), interval='1d')\n",
    "dataset = dataset[['Close']]\n",
    "\n",
    "data1 = calculate_DEMA(dataset['Close'], 10)\n",
    "data2 = calculate_TEMA(dataset['Close'], 10)\n",
    "\n",
    "normaliza_bool_update = False\n",
    "type_normalisation_update = None\n",
    "\n",
    "etl = ETL(datainput1=dataset, datainput2=data1,\n",
    "          test_size=0.2, n_input=5, timestep=5,\n",
    "          normalize_bool=normaliza_bool_update,\n",
    "          scaler=type_normalisation_update)\n",
    "\n",
    "X_train1, y_train1 = etl.X_train1, etl.y_train1\n",
    "X_train2, _ = etl.X_train2, None\n",
    "X_test1, y_test1 = etl.X_test1, etl.y_test1\n",
    "X_test2, _ = etl.X_test2, None\n",
    "\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "input_vocab_size = 10000  # Size of the input vocabulary\n",
    "target_vocab_size = 8000  # Size of the target vocabulary\n",
    "dropout_rate = 0.1\n",
    "transformer = Transformer(num_layers, num_heads, d_model, dff, input_vocab_size, target_vocab_size,\n",
    "                          pe_input=input_vocab_size, pe_target=target_vocab_size, rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Head Attention \n",
    "Final\n",
    "\n",
    "https://www.kaggle.com/code/miljan/stock-predictions-with-multi-head-attention/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, Concatenate, TimeDistributed, Lambda\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "# from keras.engine.topology import Layer\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_percentage_error, root_mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ETL:\n",
    "\n",
    "    def __init__(self, dataInput, test_size=0.2, n_input=5, timestep=5, normalize_bool=False, scaler=None) -> None:\n",
    "        self.dataInput = dataInput\n",
    "        self.test_size = test_size\n",
    "        self.n_input = n_input\n",
    "        self.timestep = timestep\n",
    "        self.normalize_bool = normalize_bool\n",
    "        self.scaler = scaler\n",
    "\n",
    "        self.dataframes = [self.extract_historic_data(\n",
    "            data) for data in dataInput]\n",
    "        self.train_data, self.test_data = self.process_dataframes()\n",
    "\n",
    "        if self.normalize_bool:\n",
    "            print('normalized', normalize_bool)\n",
    "            self.scaler = scaler\n",
    "        else:\n",
    "            print('not normalized', normalize_bool)\n",
    "            self.scaler = None\n",
    "\n",
    "    def extract_historic_data(self, datainput) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Extracts historic data and optionally normalizes it.\n",
    "        \"\"\"\n",
    "        # data = self.datainput\n",
    "        if self.normalize_bool:\n",
    "            data = self.scaler.fit_transform(\n",
    "                datainput.values.reshape(-1, 1))\n",
    "        else:\n",
    "            data = datainput\n",
    "        return data\n",
    "\n",
    "    def split_data(self, data) -> tuple:\n",
    "        if len(data) != 0:\n",
    "            train_idx = round(len(data) * (1-self.test_size))\n",
    "            train = data[:train_idx]\n",
    "            test = data[train_idx:]\n",
    "            train = np.array(train)\n",
    "            test = np.array(test)\n",
    "            # return train[:, np.newaxis], test[:, np.newaxis]\n",
    "            return np.array(train)[:, np.newaxis], np.array(test)[:, np.newaxis]\n",
    "        else:\n",
    "            raise Exception('Data set is empty, cannot split.')\n",
    "\n",
    "    def window_and_reshape(self, data) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformats data into shape our model needs,\n",
    "        namely, [# samples, timestep, # feautures]\n",
    "        samples\n",
    "        \"\"\"\n",
    "        NUM_FEATURES = 1\n",
    "        samples = int(data.shape[0] / self.timestep)\n",
    "        result = np.array(np.array_split(data, samples))\n",
    "        output = result.reshape((samples, self.timestep, NUM_FEATURES))\n",
    "        print(output.shape)\n",
    "        return output\n",
    "\n",
    "    def transform(self, train, test) -> np.array:\n",
    "        train_remainder = train.shape[0] % self.timestep\n",
    "        test_remainder = test.shape[0] % self.timestep\n",
    "        if train_remainder != 0 and test_remainder != 0:\n",
    "            train = train[train_remainder:]\n",
    "            test = test[test_remainder:]\n",
    "        elif train_remainder != 0:\n",
    "            train = train[train_remainder:]\n",
    "        elif test_remainder != 0:\n",
    "            test = test[test_remainder:]\n",
    "        # print(\"train:\", train, \"test:\", test)\n",
    "        return self.window_and_reshape(train), self.window_and_reshape(test)\n",
    "\n",
    "    def process_dataframes(self):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        for df in self.dataframes:\n",
    "            train, test = self.split_data(df)\n",
    "            train_transformed, test_transformed = self.transform(train, test)\n",
    "            train_data.append(train_transformed)\n",
    "            test_data.append(test_transformed)\n",
    "        return train_data, test_data\n",
    "\n",
    "    def to_supervised(self, data, n_out=5) -> tuple:\n",
    "        \"\"\"\n",
    "        Converts our time series prediction problem to a\n",
    "        supervised learning problem.\n",
    "        \"\"\"\n",
    "        # flatted the data\n",
    "        # data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "        data = data.reshape((data.shape[0] * data.shape[1], data.shape[2]))\n",
    "        X, y = [], []\n",
    "        in_start = 0\n",
    "        # step over the entire history one time step at a time\n",
    "        for _ in range(len(data)):\n",
    "            # define the end of the input sequence\n",
    "            in_end = in_start + self.n_input\n",
    "            out_end = in_end + n_out\n",
    "            # ensure we have enough data for this instance\n",
    "            if out_end <= len(data):\n",
    "                x_input = data[in_start:in_end, 0]\n",
    "                x_input = x_input.reshape((len(x_input), 1))\n",
    "                X.append(x_input)\n",
    "                y.append(data[in_end:out_end, 0])\n",
    "                # move along one time step\n",
    "                in_start += 1\n",
    "        print(np.array(X), np.array(y))\n",
    "        return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer='zeros', trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention():\n",
    "    def __init__(self, d_model, attn_dropout=0.1):\n",
    "        self.temper = np.sqrt(d_model)\n",
    "        self.dropout = Dropout(attn_dropout)\n",
    "\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        attn = Lambda(lambda x: K.batch_dot(\n",
    "            x[0], x[1], axes=[2, 2])/self.temper)([q, k])\n",
    "        if mask is not None:\n",
    "            mmask = Lambda(lambda x: (-1e+10)*(1-x))(mask)\n",
    "            attn = Add()([attn, mmask])\n",
    "        attn = Activation('softmax')(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = Lambda(lambda x: K.batch_dot(x[0], x[1]))([attn, v])\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention():\n",
    "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "        self.mode = mode\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = dropout\n",
    "        if mode == 0:\n",
    "            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "        elif mode == 1:\n",
    "            self.qs_layers = []\n",
    "            self.ks_layers = []\n",
    "            self.vs_layers = []\n",
    "            for _ in range(n_head):\n",
    "                # time series tensorflow\n",
    "                self.qs_layers.append(\n",
    "                    TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.ks_layers.append(\n",
    "                    TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.vs_layers.append(\n",
    "                    TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        self.layer_norm = LayerNormalization() if use_norm else None\n",
    "        self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "\n",
    "        if self.mode == 0:\n",
    "            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "            ks = self.ks_layer(k)\n",
    "            vs = self.vs_layer(v)\n",
    "\n",
    "            def reshape1(x):\n",
    "                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "                x = tf.transpose(x, [2, 0, 1, 3])\n",
    "                # [n_head * batch_size, len_q, d_k]\n",
    "                x = tf.reshape(x, [-1, s[1], d_k])\n",
    "                return x\n",
    "            qs = Lambda(reshape1)(qs)\n",
    "            ks = Lambda(reshape1)(ks)\n",
    "            vs = Lambda(reshape1)(vs)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = Lambda(lambda x: K.repeat_elements(x, n_head, 0),\n",
    "                              output_shape=lambda s: (s[0] * n_head, s[1], s[2]))(mask)\n",
    "            head, attn = self.attention(qs, ks, vs, mask=mask)\n",
    "\n",
    "            def reshape2(x):\n",
    "                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "                x = tf.reshape(x, [n_head, -1, s[1], s[2]])\n",
    "                x = tf.transpose(x, [1, 2, 0, 3])\n",
    "                # [batch_size, len_v, n_head * d_v]\n",
    "                x = tf.reshape(x, [-1, s[1], n_head*d_v])\n",
    "                return x\n",
    "            head = Lambda(reshape2)(head)\n",
    "        elif self.mode == 1:\n",
    "            heads = []\n",
    "            attns = []\n",
    "            for i in range(n_head):\n",
    "                qs = self.qs_layers[i](q)\n",
    "                ks = self.ks_layers[i](k)\n",
    "                vs = self.vs_layers[i](v)\n",
    "                head, attn = self.attention(qs, ks, vs, mask)\n",
    "                heads.append(head)\n",
    "                attns.append(attn)\n",
    "            head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "        outputs = self.w_o(head)\n",
    "        outputs = Dropout(self.dropout)(outputs)\n",
    "        if not self.layer_norm:\n",
    "            return outputs, attn\n",
    "        # outputs = Add()([outputs, q]) # sl: fix\n",
    "        return self.layer_norm(outputs), attn\n",
    "\n",
    "\n",
    "class CustomMultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, head_size, num_heads, dropout=0.2):\n",
    "        super(CustomMultiHeadAttention, self).__init__()\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=head_size)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        return self.attention(inputs, inputs, attention_mask=mask)\n",
    "\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.2, epsilon=1e-6, kernel_size=1):\n",
    "    \"\"\"\n",
    "    Creates a single transformer block.\n",
    "    \"\"\"\n",
    "    x = layers.LayerNormalization(epsilon=epsilon)(inputs)\n",
    "    mask = layers.Lambda(lambda x: tf.reduce_sum(tf.cast(x != 0, 'float32'), axis=-1, keepdims=True) != 0,\n",
    "                         output_shape=lambda s: (s[0], s[1], 1))(inputs)\n",
    "    mask = layers.Lambda(lambda x: tf.repeat(x, repeats=inputs.shape[1], axis=-1),\n",
    "                         output_shape=lambda s: (s[0], s[1], s[1]))(mask)\n",
    "\n",
    "    multi_head_attention = MultiHeadAttention(\n",
    "        num_heads, head_size, head_size, head_size, dropout)\n",
    "    x, attn = multi_head_attention(x, x, x, mask=mask)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=epsilon)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=kernel_size,\n",
    "                      activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=kernel_size)(x)\n",
    "    return x + res\n",
    "\n",
    "\n",
    "def build_transformer(head_size, num_heads, ff_dim, num_trans_blocks, mlp_units, dropout=0.2, mlp_dropout=0.2, epsilon=1e-6, kernel_size=1):\n",
    "    \"\"\"\n",
    "    Creates final model by building many transformer blocks.\n",
    "    \"\"\"\n",
    "    n_timesteps, n_features, n_outputs = 5, 1, 1  # Example values for the shape\n",
    "    inputs = [tf.keras.Input(shape=(n_timesteps, n_features))\n",
    "              for _ in range(num_trans_blocks)]\n",
    "\n",
    "    # Masking layer\n",
    "    x = [layers.Masking(mask_value=0.0)(inp) for inp in inputs]\n",
    "\n",
    "    for i in range(num_trans_blocks):\n",
    "        x[i] = transformer_encoder(x[i], head_size=head_size, num_heads=num_heads, ff_dim=ff_dim,\n",
    "                                   dropout=dropout, kernel_size=kernel_size, epsilon=epsilon)\n",
    "\n",
    "    x = [layers.GlobalAveragePooling1D(\n",
    "        data_format=\"channels_first\")(xi) for xi in x]\n",
    "    combined = layers.Concatenate()(x)\n",
    "\n",
    "    for dim in mlp_units:\n",
    "        combined = layers.Dense(dim, activation=\"relu\")(combined)\n",
    "        combined = layers.Dropout(mlp_dropout)(combined)\n",
    "\n",
    "    outputs = layers.Dense(n_outputs)(combined)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def fit_transformers(transformer: tf.keras.Model, x_train_list, y_train, x_val_list, y_val, display_loss=False):\n",
    "    transformer.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        metrics=[\"mae\", 'mape'])\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss', patience=10, restore_best_weights=True)]\n",
    "    hist = transformer.fit(x_train_list, y_train, validation_data=(\n",
    "        x_val_list, y_val), batch_size=32, epochs=25, verbose=1, callbacks=callbacks)\n",
    "    history_df = pd.DataFrame(hist.history)\n",
    "\n",
    "    if display_loss:\n",
    "        history_df['loss'].plot()\n",
    "        plt.title('Training Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train'], loc='upper right')\n",
    "        plt.show()\n",
    "        history_df['val_loss'].plot()\n",
    "        plt.title('Validation Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Validation'], loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "        # Display minimum validation loss\n",
    "        min_val_loss = history_df['val_loss'].min()\n",
    "        print(\"Minimum validation loss: {:.4f}\".format(min_val_loss))\n",
    "        # Plot mae\n",
    "        history_df.loc[:, ['mae', 'val_mae']].plot()\n",
    "        plt.title('Model MAE')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "        plt.show()\n",
    "        # Display minimum validation mae and mape\n",
    "        min_val_mae = history_df['val_mae'].min()\n",
    "        min_val_mape = history_df['val_mape'].min()\n",
    "        print(\"Minimum validation MAE: {:.4f}\".format(min_val_mae))\n",
    "        print(\"Minimum validation MAPE: {:.4f}\".format(min_val_mape))\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "class PredictAndForecast:\n",
    "    \"\"\"\n",
    "    model: tf.keras.Model\n",
    "    train: np.array\n",
    "    test: np.array\n",
    "    Takes a trained model, train, and test datasets and returns predictions\n",
    "    of len(test) with same shape.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, train_data, test_data, n_input=5, n_steps=5, scaler=None, normalize_bool=False) -> None:\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.n_input = n_input\n",
    "        self.n_steps = n_steps\n",
    "        self.scaler = scaler\n",
    "        self.normalize_bool = normalize_bool\n",
    "        self.predictions = self.get_predictions()\n",
    "\n",
    "    def forecast(self, history_data) -> np.array:\n",
    "        \"\"\"\n",
    "        Given last weeks actual data, forecasts next weeks prices.\n",
    "        \"\"\"\n",
    "        data_list = [np.array(history) for history in history_data]\n",
    "        data_list = [data.reshape(\n",
    "            (data.shape[0]*data.shape[1], data.shape[2])) for data in data_list]\n",
    "\n",
    "        predictions = []\n",
    "        for _ in range(self.n_steps):\n",
    "            input_x_list = [data[-self.n_input:, :] for data in data_list]\n",
    "            input_x_list = [input_x.reshape(\n",
    "                (1, len(input_x), input_x.shape[1])) for input_x in input_x_list]\n",
    "            yhat = self.model.predict(input_x_list, verbose=0)\n",
    "            yhat = yhat[0]\n",
    "            predictions.append(yhat)\n",
    "            data_list = [np.vstack([data, yhat]) for data in data_list]\n",
    "        return yhat\n",
    "\n",
    "    def get_predictions(self) -> np.array:\n",
    "        \"\"\"\n",
    "        compiles models predictions week by week over entire test set.\n",
    "        \"\"\"\n",
    "        history_data = [[x for x in train] for train in self.train_data]\n",
    "        predictions = []\n",
    "        for i in range(len(self.test_data[0])):\n",
    "            yhat_sequence = self.forecast(history_data)  # Recursive prediction\n",
    "            predictions.append(yhat_sequence)\n",
    "            for history, test in zip(history_data, self.test_data):\n",
    "                history.append(test[i, :])\n",
    "        predictions = np.array(predictions)\n",
    "        print(f\"Total predictions shape: {predictions.shape}\")\n",
    "        total_elements = predictions.size\n",
    "        if total_elements % self.n_steps != 0:\n",
    "            new_size = total_elements - (total_elements % self.n_steps)\n",
    "            predictions = predictions[:new_size]\n",
    "            print(f\"Trimmed predictions to new size: {new_size}\")\n",
    "\n",
    "        return predictions.reshape(-1, self.n_steps)\n",
    "\n",
    "\n",
    "def plot_results(train_list, test_list, preds, df, convertActive, normalize_bool=False, scaler=None, title_suffix=None, xlabel='AAPL stock Price'):\n",
    "    \"\"\"\n",
    "    Plots training data in blue, actual values in red, and predictions in green,\n",
    "    over time.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "    print(f'train shapes: {[train.shape for train in train_list]}')\n",
    "    print(f'test shapes: {[test.shape for test in test_list]}')\n",
    "    print(f'preds shape: {preds.shape}')\n",
    "\n",
    "    if convertActive:\n",
    "        plot_train_list = [train.squeeze(axis=-1).flatten()\n",
    "                           for train in train_list]\n",
    "        plot_test_list = [test.squeeze(axis=-1).flatten()\n",
    "                          for test in test_list]\n",
    "        plot_preds = preds.flatten()\n",
    "\n",
    "    if normalize_bool:\n",
    "        # Inverse transform to get back to original scale\n",
    "        plot_test_list = [scaler.inverse_transform(\n",
    "            plot_test.reshape(-1, 1)).reshape(plot_test.shape) for plot_test in plot_test_list]\n",
    "        plot_train_list = [scaler.inverse_transform(\n",
    "            train.reshape(-1, 1)).reshape(train.shape) for train in train_list]\n",
    "        plot_preds = scaler.inverse_transform(\n",
    "            plot_preds.reshape(-1, 1)).reshape(plot_preds.shape)\n",
    "        print(\"normalization convert active\")\n",
    "\n",
    "    plot_train = np.concatenate(plot_train_list)\n",
    "    plot_test = np.concatenate(plot_test_list)\n",
    "\n",
    "    print(f'plot_train shape: {plot_train.shape}')\n",
    "    print(f'plot_test shape: {plot_test.shape}')\n",
    "    print(f'plot_pret shape: {plot_preds.shape}')\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "\n",
    "    x_train = df[:len(plot_train)].index\n",
    "    x_test = df[len(plot_train):len(plot_train) + len(plot_test)].index\n",
    "\n",
    "    print(f'x_train shape: {len(x_train)}')\n",
    "    print(f'x_test shape: {len(x_test)}')\n",
    "    print(f'plot_train shape after concatenate: {plot_train.shape}')\n",
    "    print(f'plot_test shape after concatenate: {plot_test.shape}')\n",
    "    print(f'plot_preds shape after inverse transform: {plot_preds.shape}')\n",
    "\n",
    "    if len(plot_preds) > len(plot_test):\n",
    "        plot_preds = plot_preds[:len(plot_test)]\n",
    "    elif len(plot_preds) < len(plot_test):\n",
    "        plot_preds = np.pad(plot_preds, (0, len(\n",
    "            plot_test) - len(plot_preds)), 'constant', constant_values=np.nan)\n",
    "\n",
    "    print(f'plot_preds shape after padding: {plot_preds.shape}')\n",
    "\n",
    "    if len(x_train) != len(plot_train):\n",
    "        print(f'Length mismatch: x_train ({\n",
    "              len(x_train)}) and plot_train ({len(plot_train)})')\n",
    "    if len(x_test) != len(plot_test):\n",
    "        print(f'Length mismatch: x_test ({\n",
    "              len(x_test)}) and plot_test ({len(plot_test)})')\n",
    "\n",
    "    ax.plot(x_train, plot_train, label='Train', color='blue')\n",
    "    ax.plot(x_test, plot_test, label='Actual', color='red')\n",
    "    ax.plot(x_test, plot_preds, label='Predictions', color='green')\n",
    "    if title_suffix is None:\n",
    "        ax.set_title('Predictions vs. Actual')\n",
    "    else:\n",
    "        ax.set_title(f'Predictions vs. Actual, {title_suffix}')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel(xlabel)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class Evaluate:\n",
    "    def __init__(self, actual, predictions, normalize_bool=False, scaler=None) -> None:\n",
    "        actual = np.array(actual).flatten()\n",
    "        predictions = np.array(predictions).flatten()\n",
    "\n",
    "        # Check dimensions and reshape if necessary\n",
    "        if normalize_bool and scaler is not None:\n",
    "            actual = scaler.inverse_transform(actual.reshape(-1, 1)).flatten()\n",
    "            predictions = scaler.inverse_transform(\n",
    "                predictions.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Debugging transformed data\n",
    "        print(f\"Transformed actual: {actual[:10]}\")\n",
    "        print(f\"Transformed predictions: {predictions[:10]}\")\n",
    "        min_len = min(len(actual), len(predictions))\n",
    "        self.actual = actual[:min_len]\n",
    "        self.predictions = predictions[:min_len]\n",
    "        # Debugging lengths after transformation\n",
    "        print(f\"Final lengths - actual: {len(self.actual)\n",
    "                                         }, predictions: {len(self.predictions)}\")\n",
    "        self.var_ratio = self.compare_var()\n",
    "        self.mape = self.evaluate_model_with_mape()\n",
    "        self.rmse = self.evaluate_model_with_rmse()\n",
    "        self.mae = self.evaluate_model_with_mae()\n",
    "        self.r2 = self.evaluate_model_with_r2()\n",
    "\n",
    "    def compare_var(self):\n",
    "        return abs(1 - (np.var(self.predictions) / np.var(self.actual)))\n",
    "\n",
    "    def evaluate_model_with_mape(self):\n",
    "        mape = mean_absolute_percentage_error(self.actual, self.predictions)\n",
    "        return mape\n",
    "\n",
    "    def evaluate_model_with_rmse(self):\n",
    "        rmse = root_mean_squared_error(self.actual, self.predictions)\n",
    "        return rmse\n",
    "\n",
    "    def evaluate_model_with_mae(self):\n",
    "        mae = mean_absolute_error(self.actual, self.predictions)\n",
    "        return mae\n",
    "\n",
    "    def evaluate_model_with_r2(self):\n",
    "        r2 = r2_score(self.actual, self.predictions)\n",
    "        return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Close        DEMA        TEMA\n",
      "Date                                          \n",
      "1990-01-02    0.332589    0.332589    0.332589\n",
      "1990-01-03    0.334821    0.333327    0.333599\n",
      "1990-01-04    0.335938    0.334239    0.334771\n",
      "1990-01-05    0.337054    0.335275    0.336033\n",
      "1990-01-08    0.339286    0.336766    0.337844\n",
      "...                ...         ...         ...\n",
      "2024-07-26  217.960007  220.698230  217.263572\n",
      "2024-07-29  218.240005  219.626401  216.564154\n",
      "2024-07-30  218.800003  219.048188  216.497588\n",
      "2024-07-31  222.080002  219.737212  218.076320\n",
      "2024-08-01  218.360001  219.046155  217.562488\n",
      "\n",
      "[8712 rows x 3 columns]\n",
      "(8712, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_TEMA(data, period=10):\n",
    "    ema1 = data.ewm(span=period, adjust=False).mean()\n",
    "    ema2 = ema1.ewm(span=period, adjust=False).mean()\n",
    "    ema3 = ema2.ewm(span=period, adjust=False).mean()\n",
    "    tema = (3 * ema1) - (3 * ema2) + ema3\n",
    "    return tema.dropna()\n",
    "\n",
    "\n",
    "def calculate_DEMA(data, period=10):\n",
    "    ema1 = data.ewm(span=period, adjust=False).mean()\n",
    "    ema2 = ema1.ewm(span=period, adjust=False).mean()\n",
    "    dema = 2 * ema1 - ema2\n",
    "    return dema\n",
    "\n",
    "\n",
    "ticker = 'AAPL'\n",
    "dataset = yf.download(ticker, start='1990-01-01',\n",
    "                      end=datetime.now().strftime('%Y-%m-%d'), interval='1d')\n",
    "dataset = dataset[['Close']]\n",
    "\n",
    "data1 = calculate_DEMA(dataset['Close'], 10)\n",
    "data2 = calculate_TEMA(dataset['Close'], 10)\n",
    "\n",
    "data_inputs = [dataset, data1, data2]\n",
    "\n",
    "result = pd.DataFrame({\n",
    "    'Close': dataset['Close'],\n",
    "    'DEMA': data1,\n",
    "    'TEMA': data2\n",
    "})\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (8712, 3)\n",
      "train shape: (6970, 3) test shape: (1742, 3)\n",
      "Train shape: (6970, 3)\n",
      "Test shape: (1740, 3)\n",
      "(1394, 5, 3)\n",
      "(348, 5, 3)\n",
      "[[[ 0.332589  ]\n",
      "  [ 0.33482099]\n",
      "  [ 0.33593801]\n",
      "  [ 0.33705401]\n",
      "  [ 0.339286  ]]\n",
      "\n",
      " [[ 0.33482099]\n",
      "  [ 0.33593801]\n",
      "  [ 0.33705401]\n",
      "  [ 0.339286  ]\n",
      "  [ 0.33593801]]\n",
      "\n",
      " [[ 0.33593801]\n",
      "  [ 0.33705401]\n",
      "  [ 0.339286  ]\n",
      "  [ 0.33593801]\n",
      "  [ 0.32142901]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[39.36999893]\n",
      "  [39.96250153]\n",
      "  [40.40000153]\n",
      "  [40.23749924]\n",
      "  [39.46500015]]\n",
      "\n",
      " [[39.96250153]\n",
      "  [40.40000153]\n",
      "  [40.23749924]\n",
      "  [39.46500015]\n",
      "  [39.375     ]]\n",
      "\n",
      " [[40.40000153]\n",
      "  [40.23749924]\n",
      "  [39.46500015]\n",
      "  [39.375     ]\n",
      "  [39.30250168]]] [[ 0.33593801  0.32142901  0.308036    0.308036    0.30580401]\n",
      " [ 0.32142901  0.308036    0.308036    0.30580401  0.31138399]\n",
      " [ 0.308036    0.308036    0.30580401  0.31138399  0.296875  ]\n",
      " ...\n",
      " [39.375      39.30250168 39.94499969 39.99499893 39.81750107]\n",
      " [39.30250168 39.94499969 39.99499893 39.81750107 39.96500015]\n",
      " [39.94499969 39.99499893 39.81750107 39.96500015 40.36750031]]\n",
      "[[[ 40.72750092]\n",
      "  [ 40.83750153]\n",
      "  [ 41.        ]\n",
      "  [ 41.01250076]\n",
      "  [ 40.52000046]]\n",
      "\n",
      " [[ 40.83750153]\n",
      "  [ 41.        ]\n",
      "  [ 41.01250076]\n",
      "  [ 40.52000046]\n",
      "  [ 40.47750092]]\n",
      "\n",
      " [[ 41.        ]\n",
      "  [ 41.01250076]\n",
      "  [ 40.52000046]\n",
      "  [ 40.47750092]\n",
      "  [ 40.31499863]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[234.3999939 ]\n",
      "  [234.82000732]\n",
      "  [228.88000488]\n",
      "  [224.17999268]\n",
      "  [224.30999756]]\n",
      "\n",
      " [[234.82000732]\n",
      "  [228.88000488]\n",
      "  [224.17999268]\n",
      "  [224.30999756]\n",
      "  [223.96000671]]\n",
      "\n",
      " [[228.88000488]\n",
      "  [224.17999268]\n",
      "  [224.30999756]\n",
      "  [223.96000671]\n",
      "  [225.00999451]]] [[ 40.47750092  40.31499863  39.65750122  40.375       40.21500015]\n",
      " [ 40.31499863  39.65750122  40.375       40.21500015  39.91249847]\n",
      " [ 39.65750122  40.375       40.21500015  39.91249847  39.56999969]\n",
      " ...\n",
      " [223.96000671 225.00999451 218.53999329 217.49000549 217.96000671]\n",
      " [225.00999451 218.53999329 217.49000549 217.96000671 218.24000549]\n",
      " [218.53999329 217.49000549 217.96000671 218.24000549 218.80000305]]\n",
      "not normalized False\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling LayerNormalization.call().\n\n\u001b[1mCannot reshape a tensor with 3 elements to shape [1,1,1] (1 elements) for '{{node functional_9_1/layer_normalization_39_1/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](functional_9_1/layer_normalization_39_1/Reshape/ReadVariableOp, functional_9_1/layer_normalization_39_1/Reshape/shape)' with input shapes: [3], [3] and with input tensors computed as partial shapes: input[1] = [1,1,1].\u001b[0m\n\nArguments received by LayerNormalization.call():\n  • inputs=tf.Tensor(shape=(None, 5, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mX_test, data\u001b[38;5;241m.\u001b[39my_test\n\u001b[1;32m     10\u001b[0m transformer \u001b[38;5;241m=\u001b[39m build_transfromer(head_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, ff_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_trans_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     11\u001b[0m                                 mlp_units\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m256\u001b[39m], mlp_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.10\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.10\u001b[39m, attention_axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                       \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m transformer_preds \u001b[38;5;241m=\u001b[39m PredictAndForecast(transformer, data\u001b[38;5;241m.\u001b[39mtrain, data\u001b[38;5;241m.\u001b[39mtest, n_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     17\u001b[0m                                        scaler\u001b[38;5;241m=\u001b[39mtype_normalisation_update, normalize_bool\u001b[38;5;241m=\u001b[39mnormaliza_bool_update)\n\u001b[1;32m     19\u001b[0m plot_results(data\u001b[38;5;241m.\u001b[39mtrain, data\u001b[38;5;241m.\u001b[39mtest, transformer_preds\u001b[38;5;241m.\u001b[39mpredictions,\n\u001b[1;32m     20\u001b[0m              data\u001b[38;5;241m.\u001b[39mdf, normalize_bool\u001b[38;5;241m=\u001b[39mnormaliza_bool_update, scaler\u001b[38;5;241m=\u001b[39mtype_normalisation_update, title_suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer Apple - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_type_used[j]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 186\u001b[0m, in \u001b[0;36mfit_transformer\u001b[0;34m(transformer, x_train, y_train, X_val, y_val, display_loss)\u001b[0m\n\u001b[1;32m    183\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m    184\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# hist = transformer.fit(data.X_train, data.y_train, batch_size=32, epochs=25, verbose=1, callbacks=callbacks)\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m history_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(hist\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display_loss:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling LayerNormalization.call().\n\n\u001b[1mCannot reshape a tensor with 3 elements to shape [1,1,1] (1 elements) for '{{node functional_9_1/layer_normalization_39_1/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](functional_9_1/layer_normalization_39_1/Reshape/ReadVariableOp, functional_9_1/layer_normalization_39_1/Reshape/shape)' with input shapes: [3], [3] and with input tensors computed as partial shapes: input[1] = [1,1,1].\u001b[0m\n\nArguments received by LayerNormalization.call():\n  • inputs=tf.Tensor(shape=(None, 5, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "normaliza_bool_update = False\n",
    "type_normalisation_update = None\n",
    "\n",
    "data = ETL(datainput=result, test_size=0.2, n_input=5, timestep=5,\n",
    "           normalize_bool=normaliza_bool_update, scaler=type_normalisation_update)\n",
    "x_train, y_train = data.X_train, data.y_train\n",
    "X_val, y_val = data.X_test, data.y_test\n",
    "\n",
    "\n",
    "transformer = build_transfromer(head_size=128, num_heads=4, ff_dim=2, num_trans_blocks=4,\n",
    "                                mlp_units=[256], mlp_dropout=0.10, dropout=0.10, attention_axes=1)\n",
    "\n",
    "hist = fit_transformer(transformer, x_train,\n",
    "                       y_train, X_val, y_val, display_loss=False)\n",
    "\n",
    "transformer_preds = PredictAndForecast(transformer, data.train, data.test, n_input=5,\n",
    "                                       scaler=type_normalisation_update, normalize_bool=normaliza_bool_update)\n",
    "\n",
    "plot_results(data.train, data.test, transformer_preds.predictions,\n",
    "             data.df, normalize_bool=normaliza_bool_update, scaler=type_normalisation_update, title_suffix=f'Transformer Apple - {list_type_used[j]}')\n",
    "train_evaluation = Evaluate(data.test, transformer_preds.predictions,\n",
    "                            normalize_bool=normaliza_bool_update, scaler=type_normalisation_update)\n",
    "print(f\"MAPE for : {train_evaluation.mape}\")\n",
    "print(f\"RMSE for : {train_evaluation.rmse}\")\n",
    "print(f\"MAE for : {train_evaluation.mae}\")\n",
    "print(f\"R-squared for : {train_evaluation.r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ljhbkuhkjhjh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1394, 5, 1)\n",
      "(348, 5, 1)\n",
      "(1394, 5, 1)\n",
      "(348, 5, 1)\n",
      "(1394, 5, 1)\n",
      "(348, 5, 1)\n",
      "not normalized False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Initialize the ETL process\u001b[39;00m\n\u001b[1;32m     27\u001b[0m etl \u001b[38;5;241m=\u001b[39m ETL(data_inputs, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     28\u001b[0m           normalize_bool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, scaler\u001b[38;5;241m=\u001b[39mMinMaxScaler())\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43metl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataframes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Process the data into supervised learning format\u001b[39;00m\n\u001b[1;32m     31\u001b[0m X_train_list, y_train_list \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def calculate_TEMA(data, period=10):\n",
    "    ema1 = data.ewm(span=period, adjust=False).mean()\n",
    "    ema2 = ema1.ewm(span=period, adjust=False).mean()\n",
    "    ema3 = ema2.ewm(span=period, adjust=False).mean()\n",
    "    tema = (3 * ema1) - (3 * ema2) + ema3\n",
    "    return tema.dropna()\n",
    "\n",
    "\n",
    "def calculate_DEMA(data, period=10):\n",
    "    ema1 = data.ewm(span=period, adjust=False).mean()\n",
    "    ema2 = ema1.ewm(span=period, adjust=False).mean()\n",
    "    dema = 2 * ema1 - ema2\n",
    "    return dema\n",
    "\n",
    "\n",
    "ticker = 'AAPL'\n",
    "dataset = yf.download(ticker, start='1990-01-01',\n",
    "                      end=datetime.now().strftime('%Y-%m-%d'), interval='1d')\n",
    "dataset = dataset[['Close']]\n",
    "\n",
    "data1 = calculate_DEMA(dataset['Close'], 10)\n",
    "data2 = calculate_TEMA(dataset['Close'], 10)\n",
    "\n",
    "data_inputs = [dataset, data1, data2]\n",
    "\n",
    "# Initialize the ETL process\n",
    "etl = ETL(data_inputs, test_size=0.2,\n",
    "          normalize_bool=False, scaler=MinMaxScaler())\n",
    "print(etl.dataframes.shape)\n",
    "# Process the data into supervised learning format\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "for train, test in zip(etl.train_data, etl.test_data):\n",
    "    X_train, y_train = etl.to_supervised(train)\n",
    "    X_test, y_test = etl.to_supervised(test)\n",
    "    X_train_list.append(X_train)\n",
    "    y_train_list.append(y_train)\n",
    "    X_test_list.append(X_test)\n",
    "    y_test_list.append(y_test)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train_list = [np.array(x) for x in X_train_list]\n",
    "X_test_list = [np.array(x) for x in X_test_list]\n",
    "y_train = y_train_list[0]  # Assuming both datasets have the same y\n",
    "y_test = y_test_list[0]  # Assuming both datasets have the same y\n",
    "\n",
    "# Build the transformer model\n",
    "model = build_transformer(\n",
    "    head_size=64,\n",
    "    num_heads=4,\n",
    "    ff_dim=64,\n",
    "    num_trans_blocks=len(data_inputs),\n",
    "    mlp_units=[128],\n",
    "    dropout=0.2,\n",
    "    mlp_dropout=0.2\n",
    ")\n",
    "\n",
    "# Fit the transformer model\n",
    "fit_transformers(model, X_train_list, y_train,\n",
    "                 X_test_list, y_test, display_loss=True)\n",
    "\n",
    "predict_and_forecast = PredictAndForecast(\n",
    "    model=model,\n",
    "    train_data=X_train_list,\n",
    "    test_data=X_test_list,\n",
    "    n_input=5,\n",
    "    n_steps=5,\n",
    "    scaler=MinMaxScaler(),\n",
    "    normalize_bool=False\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "predictions = predict_and_forecast.predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(etl.test_data, [predictions],\n",
    "                    normalize_bool=False, scaler=MinMaxScaler())\n",
    "print(f\"MAPE: {evaluate.mape}\")\n",
    "print(f\"RMSE: {evaluate.rmse}\")\n",
    "print(f\"MAE: {evaluate.mae}\")\n",
    "print(f\"R2: {evaluate.r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "# plot_results(train=X_train_list, test=X_test_list, preds=predictions, df=dataset, convertActive=True,\n",
    "#              normalize_bool=False, scaler=MinMaxScaler(), title_suffix=\"AAPL\", xlabel='AAPL stock Price')\n",
    "plot_results(train=etl.train_data, test=etl.test_data, preds=predictions, df=dataset, convertActive=True,\n",
    "             normalize_bool=False, scaler=MinMaxScaler(), title_suffix=\"AAPL\", xlabel='AAPL stock Price')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
